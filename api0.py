from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI

# Initialize FastAPI app
app = FastAPI()

# Set your OpenAI API key
client = OpenAI(api_key = "LL-XupZC6EcU1K9u45GVXrQvQJMzxkcmdTyfV7kCQ295jDQQ3v11IKEk8i5q0oapBPM", base_url = "https://api.llama-api.com")

# Function to extract answer from the GPT-3.5 response
import re
def extract_answer(response_text: str) -> str:
    match = re.search(r'enign', response_text)
    if match:
        return 'benign'
    else:
        match = re.search(r'alware', response_text)
        if match:
            return 'malware'
        else:
            return 'unknown'

# Define the zero_labeller function
def zero_labeller(test: str) -> str:
    response = client.chat.completions.create(
        model="llama3-70b",
        messages=[
            {"role": "system", "content": f"The following text is the list of permissions. Text: {test}\nPlease label it ***benign*** or ***malware***?"}
        ]
    )
    res = response.choices[0].message.content
    label_binary = extract_answer(res)
    return label_binary

# Define the request body model
class TextRequest(BaseModel):
    text: str

# Create an endpoint to label text
@app.post("/label/")
async def label_text(request: TextRequest):
    try:
        label = zero_labeller(request.text)
        return {"label": label}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
